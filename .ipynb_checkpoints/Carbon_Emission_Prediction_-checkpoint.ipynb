{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "855156cc-ce22-473b-898e-640239e2702a",
   "metadata": {
    "id": "855156cc-ce22-473b-898e-640239e2702a"
   },
   "source": [
    "## Air Quality Prediction from Low-Cost IoT devices\n",
    "Data Scientist: Victor Kelechi Ahaji\n",
    "\n",
    "`Objective:`\n",
    "Developing a machine learning model that accurately predicts CO2 levels using data from Chemotronixâ€™s low-cost sensors. Building this model will help bridge the gap between affordability and precision in carbon emission tracking enabling widespread adoption of low-cost monitoring technologies.\n",
    "\n",
    "`Expected Result:`\n",
    "> 1.) Democratize access to environmental monitoring tools.\n",
    "\n",
    "> 2.) Assist governments and organizations in implementing data-driven policies to curb carbon emissions.\n",
    "\n",
    "> 3.) Promote sustainability by making emission tracking affordable for communities and industries worldwide.\n",
    "\n",
    "`Evaluation:`\n",
    "The evaluation metric for this competition is Root Mean Squared Error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3483bf5-0728-4828-9196-3e9395178de9",
   "metadata": {
    "id": "c3483bf5-0728-4828-9196-3e9395178de9"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "597af307-fdc9-459e-9a54-1bcabc2eb213",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "597af307-fdc9-459e-9a54-1bcabc2eb213",
    "outputId": "57a94f3a-98c4-408a-d48c-17fb5d8e596b"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "C extension: None not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext' to build the C extensions first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# numpy compat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     from pandas.compat import (\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mis_numpy_dev\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_is_numpy_dev\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\compat\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompressors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_numpy_dev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m from pandas.compat.pyarrow import (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\compat\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m_nlv\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_min_numpy_ver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     raise ImportError(\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;34mf\"this version of pandas is incompatible with numpy < {_min_numpy_ver}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: this version of pandas is incompatible with numpy < 1.22.4\nyour numpy version is 1.21.0.\nPlease upgrade numpy to >= 1.22.4 to use this pandas version",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1600\\1840293101.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_err\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0m_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m     raise ImportError(\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;34mf\"C extension: {_module} not built. If you want to import \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;34m\"pandas from the source directory, you may need to run \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: C extension: None not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext' to build the C extensions first."
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49359935-8084-4833-abcd-10621d39a4c7",
   "metadata": {
    "id": "49359935-8084-4833-abcd-10621d39a4c7"
   },
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b8bc5-2464-44fc-8f88-cfe33ae7abe5",
   "metadata": {
    "id": "f57b8bc5-2464-44fc-8f88-cfe33ae7abe5"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv(\"Train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "sample_submission = pd.read_csv(\"SampleSubmission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8d105c-846d-457e-9e84-89db26291197",
   "metadata": {
    "id": "9b8d105c-846d-457e-9e84-89db26291197"
   },
   "source": [
    "### Quick Look\n",
    "Taking an initial look on the dataset to understand structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98069c0-32d8-4977-8d98-5a7bc40e2349",
   "metadata": {
    "id": "b98069c0-32d8-4977-8d98-5a7bc40e2349"
   },
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d032ee2-a44d-448c-ae70-336a00439df3",
   "metadata": {
    "id": "9d032ee2-a44d-448c-ae70-336a00439df3"
   },
   "outputs": [],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ed0872-4870-41a9-843a-53323d9c2e83",
   "metadata": {
    "id": "a3ed0872-4870-41a9-843a-53323d9c2e83"
   },
   "outputs": [],
   "source": [
    "# Descriptive statistic of the train dataset\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88844bcd-0197-4173-be9f-a60c440a7c93",
   "metadata": {
    "id": "88844bcd-0197-4173-be9f-a60c440a7c93"
   },
   "source": [
    "From the above, we can understand that we have about 7307 samples in our train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587e0755-4711-4f0a-ac8f-8c303884c454",
   "metadata": {
    "id": "587e0755-4711-4f0a-ac8f-8c303884c454"
   },
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0d5d93-5c3f-43d8-8897-b834fae155a8",
   "metadata": {
    "id": "fa0d5d93-5c3f-43d8-8897-b834fae155a8"
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec5b38-e08d-45c7-9f34-6556285266e6",
   "metadata": {
    "id": "51ec5b38-e08d-45c7-9f34-6556285266e6"
   },
   "outputs": [],
   "source": [
    "train.set_index(\"ID\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f095fc-28e9-43ba-a7e5-176d071a4682",
   "metadata": {
    "id": "d6f095fc-28e9-43ba-a7e5-176d071a4682"
   },
   "source": [
    "From the above we can observe the absence of null values in the features. But for one of the columns; the `device_name` feature might not be represented in the right data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1fe72c-41cd-4ed8-be7c-8526bb7f9eae",
   "metadata": {
    "id": "ea1fe72c-41cd-4ed8-be7c-8526bb7f9eae"
   },
   "outputs": [],
   "source": [
    "print(train[\"device_name\"].nunique())\n",
    "print(train[\"device_name\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4e1dde-8771-4ba8-8e08-9e3bd4dac3c0",
   "metadata": {
    "id": "ad4e1dde-8771-4ba8-8e08-9e3bd4dac3c0"
   },
   "source": [
    "The `device_name` feature should be more of a category data type instead of object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66313e3a-c7e4-4b21-9231-bf9fc7030cdc",
   "metadata": {
    "id": "66313e3a-c7e4-4b21-9231-bf9fc7030cdc"
   },
   "outputs": [],
   "source": [
    "# Change the data type from `object` to `category` data type\n",
    "train[\"device_name\"] = train[\"device_name\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec562f-f73c-48ed-af67-883ea3ad8845",
   "metadata": {
    "id": "74ec562f-f73c-48ed-af67-883ea3ad8845"
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23272193-58c5-4ede-ab6d-6ba89f20523a",
   "metadata": {
    "id": "23272193-58c5-4ede-ab6d-6ba89f20523a"
   },
   "source": [
    "### Understanding the distribution of train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068b016-0136-4369-af35-eb6ab390b3f4",
   "metadata": {
    "id": "1068b016-0136-4369-af35-eb6ab390b3f4"
   },
   "outputs": [],
   "source": [
    "# Utilizing skew module of the scipy library\n",
    "skewness_value = skew(train.select_dtypes(include=[\"number\"]))\n",
    "Skewness = list(zip(train.select_dtypes(include = [\"number\"]).columns, skewness_value))\n",
    "for col, skewness_value in Skewness:\n",
    "    print(f'Skewness of {col} : {skewness_value:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6138a1cf-0743-463b-9eab-95673e83ee42",
   "metadata": {
    "id": "6138a1cf-0743-463b-9eab-95673e83ee42"
   },
   "outputs": [],
   "source": [
    "# Visualizing to understand skewness\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# create histograms for each numeric column\n",
    "fig, ax = plt.subplots(nrows= 3, ncols= 3, figsize =(15,10))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i, col in enumerate(train.select_dtypes(include=[\"number\"]).columns):\n",
    "    sns.histplot(train[col], bins = 10, kde = True, ax = ax[i], color = 'blue')\n",
    "    ax[i].set_title(f'Histogram of {col}')\n",
    "    ax[i].set_xlabel(col)\n",
    "    ax[i].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0fc134-89b2-4c44-8765-18aecbf973a3",
   "metadata": {
    "id": "7d0fc134-89b2-4c44-8765-18aecbf973a3"
   },
   "source": [
    "> Based on the above plot and skewness values, we can comment that the dataset is quite normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f363798b-8f9f-4656-b9fa-74c6007c6af0",
   "metadata": {
    "id": "f363798b-8f9f-4656-b9fa-74c6007c6af0"
   },
   "source": [
    "### Understanding the nature of relationship between features.\n",
    "This is to help avoid multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e1a42-0055-41d6-9df6-9de2127d58f0",
   "metadata": {
    "id": "475e1a42-0055-41d6-9df6-9de2127d58f0"
   },
   "outputs": [],
   "source": [
    "# Using seabon pairplot\n",
    "sns.pairplot(train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5ec5e0-e822-49c1-b8d2-dd508883dcb9",
   "metadata": {
    "id": "9e5ec5e0-e822-49c1-b8d2-dd508883dcb9"
   },
   "outputs": [],
   "source": [
    "# Using heatmap to understand correlation\n",
    "plt.figure(figsize = (15,10))\n",
    "sns.heatmap(train.drop(\"device_name\", axis = 1).corr(), annot = True)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2d515-27a8-4564-b363-8cf3a7ba3782",
   "metadata": {
    "id": "e5a2d515-27a8-4564-b363-8cf3a7ba3782"
   },
   "source": [
    "### Feature Engineering, Selection and Preprocessing\n",
    "Based on the above, we have instances of multicollinearity this requires us to drop certain features before model building, but we will not drop these features. An algorithm that penalizes multicollineraity will be utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kMYGBuKkaGc6",
   "metadata": {
    "id": "kMYGBuKkaGc6"
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# Create Heat Index Feature\n",
    "train[\"Heat_Index\"] = train[\"Temperature\"] + (0.55 - 0.0055 * train[\"Humidity\"]) * (train[\"Temperature\"] - 14.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0713252-d849-414c-9302-5615a51390b4",
   "metadata": {
    "id": "f0713252-d849-414c-9302-5615a51390b4"
   },
   "outputs": [],
   "source": [
    "# Feature selection and preprocessing\n",
    "features = train.drop(\"CO2\", axis = 1).columns\n",
    "target =  'CO2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5c2ee-64e6-4d40-be57-1c2ce5cc53e8",
   "metadata": {
    "id": "a3d5c2ee-64e6-4d40-be57-1c2ce5cc53e8"
   },
   "outputs": [],
   "source": [
    "X = train[features]\n",
    "y = train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cef550-9442-4d1b-b879-431754b991b0",
   "metadata": {
    "id": "a5cef550-9442-4d1b-b879-431754b991b0"
   },
   "outputs": [],
   "source": [
    "# Feature transformation\n",
    "numerical_cols = X.select_dtypes(include=[\"number\"]).columns\n",
    "categorical_col = X.columns.drop(numerical_cols)\n",
    "\n",
    "# Log Transformation (for positive skewness)\n",
    "log_transformer = FunctionTransformer(lambda x: np.log1p(x), validate=False)\n",
    "\n",
    "# Squared Transformation (for negative skewness)\n",
    "squared_transformer = FunctionTransformer(lambda x: x**2, validate=False)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# One-Hot Encoding for Categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# PCA for dimensionality reduction\n",
    "pca = PCA(n_components = 6)\n",
    "\n",
    "# Column Transfprmer to apply transformation\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"log\", log_transformer, [\"MG811_analog\", \"MQ9_analog\",\"Humidity\",\"MQ7_analog\"]),  # Apply log transformation to positively skewed features\n",
    "    (\"square\", squared_transformer, [\"MQ135_analog\",\"Temperature\"]),  # Apply squared transformation to negatively skewed features\n",
    "    (\"scale\", scaler, numerical_cols),  # Standardization for all numeric features\n",
    "    (\"one_hot\", encoder, categorical_col),  # One-Hot Encoding\n",
    "])\n",
    "\n",
    "# Create a Pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"pca\", pca)\n",
    "])\n",
    "\n",
    "# Apply Transformations\n",
    "X_processed = pipeline.fit_transform(X)\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "X_processed = pd.DataFrame(X_processed)\n",
    "X_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qg5TzHWZjH27",
   "metadata": {
    "id": "Qg5TzHWZjH27"
   },
   "outputs": [],
   "source": [
    "# Data Splitting\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d4cd02-2727-4e42-853c-f859e0b25207",
   "metadata": {
    "id": "80d4cd02-2727-4e42-853c-f859e0b25207"
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7552d6-bfa3-4dc3-8c6d-64045a96018f",
   "metadata": {
    "id": "3d7552d6-bfa3-4dc3-8c6d-64045a96018f"
   },
   "outputs": [],
   "source": [
    "# Model Training:Deep Learning Model\n",
    "reg_model = RandomForestRegressor(n_estimators = 600, max_depth = 10\n",
    "                                  , min_samples_split = 2, min_samples_leaf = 2,\n",
    "                                  max_features = \"sqrt\", bootstrap = True)\n",
    "reg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00697465-d2ab-43d6-b25c-b9aa76034633",
   "metadata": {
    "id": "00697465-d2ab-43d6-b25c-b9aa76034633"
   },
   "outputs": [],
   "source": [
    "# Predicting\n",
    "y_val_pred = reg_model.predict(X_val)\n",
    "\n",
    "# Evaluating\n",
    "RMSE = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "print(f'Validation RMSE: {RMSE:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oCDvfvCvqpAT",
   "metadata": {
    "id": "oCDvfvCvqpAT"
   },
   "outputs": [],
   "source": [
    "reg_model_ = GradientBoostingRegressor(n_estimators = 100,learning_rate = 0.1,\n",
    "                                       max_depth = 7, min_samples_split = 5,\n",
    "                                       min_samples_leaf = 4, subsample = 0.8, max_features = \"sqrt\")\n",
    "reg_model_.fit(X_train, y_train)\n",
    "\n",
    "# Predicting\n",
    "y_val_pred_ = reg_model_.predict(X_val)\n",
    "\n",
    "# Evaluating\n",
    "RMSE = np.sqrt(mean_squared_error(y_val, y_val_pred_))\n",
    "print(f'Validation RMSE: {RMSE:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nhMTtFvHqPjt",
   "metadata": {
    "id": "nhMTtFvHqPjt"
   },
   "outputs": [],
   "source": [
    "# Create the Heat Index feature in the test data\n",
    "test[\"Heat_Index\"] = test[\"Temperature\"] + (0.55 - 0.0055 * test[\"Humidity\"]) * (test[\"Temperature\"] - 14.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CDxozEOzITZI",
   "metadata": {
    "id": "CDxozEOzITZI"
   },
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "test_features = pipeline.fit_transform(test[features])\n",
    "test_predictions = reg_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U_vXDLSFITPf",
   "metadata": {
    "id": "U_vXDLSFITPf"
   },
   "outputs": [],
   "source": [
    "# Prepare submission\n",
    "sample_submission['CO2'] = test_predictions\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file saved as 'submission.csv'\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
